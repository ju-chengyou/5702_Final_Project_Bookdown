[
["index.html", "EDAV Final Project Group 23 Chapter 1 Introduction", " EDAV Final Project Group 23 Shaofeng Wu, Chengyou Ju, Mingrui Liu, Yujing Song 2019-12-13 Chapter 1 Introduction This project is created by Shaofeng Wu (sw3428), Chengyou Ju (cj2624), Mingrui Liu (ml4404), Yujing Song (ys3251) for STAT GR5702 Final Project Group 23. New York, a charming place which is attracting people all around city, has thousands of opportunities for people who have a dream to work in the city. In this way, New York government is posting different jobs every day so that the posted jobs can meet different requirements of different groups of people. Under this circumstance, it’s interesting to analyze the trends of the job postings in the NYC. For instance, analyzing the areas and required skills of job postings can help people recognize the right places to find the right jobs. Finding the trends of salaries vs. job categories also provides people a fabulous tool to have a sense on which jobs are highly paid jobs so that they can try their best to learn more required skills and earn more money. As graduate students who are actively seeking jobs and internships, we come up with the idea that we can choose a topic related to the current employment condition in New York. After doing some research, we decide to focus on studying the recent job postings in New York. We want to see, for instance, salary ranges for different categories of jobs, what skills are preferred by a particular kind of jobs, or if there is a relationship between job locations and salaries. This is the link of GitHub that contains all our work for the project https://github.com/Alexsfwu/5702-final-project. "],
["data-sources.html", "Chapter 2 Data Sources", " Chapter 2 Data Sources Our data come from https://data.cityofnewyork.us/City-Government/NYC-Jobs/kpav-sd4t, which contains current job postings available on the City of New York’s official jobs site http://www.nyc.gov/html/careers/html/search/search.shtml. Mingrui is responsible for looking for and collecting the data. Since the data can be directly exported from the website as a .csv file, there is no major obstacle for us when gathering the data. After downloading the data from the website, we use the built-in read.csv method to read and store the data as a data frame for future manipulation. This data frame has 3040 observations of 28 variables. Each entry represents a job posting on the website, while the columns represent the information about each job, including Business Title, Salary Range, Job Description, etc. The only problem about the data is that in the origin .csv file, there are plenty of empty entries. Therefore, when we read data from the file and store as a data frame, we fill all these blank entries with NAs. "],
["data-transformation.html", "Chapter 3 Data Transformation", " Chapter 3 Data Transformation Since the dataset is stored in a neat .csv file, we simply read the file using the read.csv method and store the data in the data frame job. Later, when we need some specific attributes of the job postings dataset, we can use simply $ or pipes and the %&gt;% operator to extract the columns we need. In the dataset we find there is some duplicated recordings, so we removed them first. In order to plot a interactive map, we need the data of coordinates. However, we don’t have those coordinates in our dataset. Under this circumstance, we deployed the google API which is so-called geocode(). After we successfully scraped the longitudes and latitudes of addresses of different job locations, we were able to plot the points on the map of NYC accurately. What’s more, because the google API could not find some specific locations and returned NA, we had to drop those rows with NA values in longitudes and latitudes. "],
["missing-values.html", "Chapter 4 Missing Values", " Chapter 4 Missing Values As mentioned above, we fill all empty slots in the original dataset with NAs. Then we draw a graph using extracat::visna to visualize the missing patterns of our dataset. The columns represent the 28 variables and the rows the missing patterns. The cells for the variables with missing values in a pattern are drawn in blue. The variables and patterns have been ordered by numbers of missings on both rows and columns. The bars beneath the columns show the proportions of missings by variable and the bars on the right show the relative frequencies of patterns. From the plot, we can tell that all values under the column Recruitment Contact are missing. Therefore, we decide to drop this column from our data frame. For other columns, we will keep these NAs for now, but we try not to make our analysis depend heavily on the column Hours.shift ,Post.until and Work Location 1 because those columns have lots of missings. Furthermore, we need to be careful when to delete NA values. Because in our dataset we have lots of categorical variables and NA in those categorical variables might be one of the factor levels. For example in the level variable, the NA values mean entry-level and we transformed those NA values as one-factor level. "],
["results.html", "Chapter 5 Results 5.1 Job Count by Category 5.2 Distributions of Salaries by Payroll Types 5.3 Distribution of Salaries by Categories 5.4 Job Postings Count Trend In One Year 5.5 Word Clouds for Text Information 5.6 More Studies on Tech Jobs", " Chapter 5 Results 5.1 Job Count by Category Since we want to study the total number of jobs for each job category and one particular job could belong to multiple categories, we extract all the categories related to a job, seperate them, and create a new data frame called popular_category, which stores the counts of different job categories. Then, in order to visualize the numbers of job postings among different categories, we draw a descending horizontal bar chart based on his new data frame. From the graphs below, we can tell that Architecture and Engineering have the most job postings, while Procurement Policy and Social Services have the fewest. 5.2 Distributions of Salaries by Payroll Types We also want to study the distributions of salaries among different types of payroll. Since there are three payroll types in our data set, which are Annual, Daily and Hourly, we will draw three histograms to visualize the distributions. We take the mean of Salary Range From and Salary Range To as our salary for the histogram at the x-axis. From these three plots above, we have the following obeservations: 1. For most of the jobs, the salaries are given annually. There are also some jobs which have hourly salaries. Only a few of those jobs have daily salaries. 2. For salaries calculated annually, it has approximately right-skewed normal distribution, which means that most jobs do not have a relatively high salaries. 3. For salaries calculated daily, there is no specific pattern regarding the distribution. Some jobs have relatively low daily salaries, while others have much higher salaries. 4. For salaries calculated hourly, most of them has a relatively low value, but there are still some jobs have relatively high hourly salaries. Then, We also look into our data and find out more information about our salary distribution. For insace, for houly paied jobs, Stationary Engineer and City Medical Specialist have extremly high hourly salaries, while College Aide has low hourly salaries. 5.3 Distribution of Salaries by Categories This boxplot gives us a general idea of the salary distribution for different kinds of jobs from the highest to the lowest mean salaries. For instance, we can see that jobs of Building Operations &amp; Maintenance, in general, have lower salaries than those of Information Technology &amp; Telecommunications. We can also see a general pattern that the higher the Annual Salary, the wider the range of the Salaries. 5.4 Job Postings Count Trend In One Year The plot above shows us the change in the number of postings of popular categories from January to December. As we can see, most of the job postings are posted between August to December, which makes sense because most recruiting season happens during the fall. We can also see that some of the job categories have demand in other seasons. For example, Public Safty, Inspections, and Enforcement also have some recruiting demand in May. Some of the job categories have stable recruiting demand throughout the whole year, such as Maintenance, Architecture, and Engineering. 5.5 Word Clouds for Text Information 5.5.1 How we get started Meanwhile, we also want to study the minimum qualification requirements and preferred skills for the available jobs in NYC. We want to find if there are any patterns in these two columns and if we can extract any useful information from them. In order to illustrate our findings graphfically, we decide to use Word Clouds to show the most frequent words in these texts. So what is Word Clouds? Word Clouds is visual representations of text data. They are useful for quickly perceiving the most prominent terms, which makes them widely used in media and well understood by the public. A Word Cloud is a collection of words depicted in different sizes. The bigger and bolder the word appears, the greater frequency within a given text and the more important it is. In order to extract meaningful vocabularies from the text descriptions, we take advantage of the text mining package tm in R. This package is based on the ideas of Natural Language Processing (NLP). It have methods that can tranform all words to lowercases, remove words that are uninformative in Enlighs such as “a” and “the”, and get rid of whitespaces and punctuations. After these manipulations on the text data, we can create a new data frame of word frequencies. We can also sort it by frequency and find out the most frequent words under minimum qualification requirements and preferred skills for all jobs or for any particular category of jobs that we are interested in. 5.5.2 Results Due to the problem of wordcloud2 that only one Word Cloud graph appears after knitting to Bookdown or HTML, we save all our graphs to four seperate html files that can be automatically rendered everytime they are opened in a browser. Here are the link to those files in my GitHub repo: https://github.com/ju-chengyou/5702_Final_Word_Cloud. Here, we will show the Word Cloud of the most frequent words in Minimum Qual Requirements among all jobs in our dataset. 5.5.2.1 Minium Qual Requirements @ All Jobs Minimum Qual Requirements in All Jobs Word Frequency Word Frequency experience 13182 years 6907 college 4875 accredited 4828 education 4567 satisfactory 4550 described 4382 fulltime 4064 degree 3931 equivalent 3471 assignment 2860 least 2780 school 2717 four 2535 engineering 2327 candidates 2242 professional 2222 level 2165 baccalaureate 2141 high 2083 5.5.2.2 Preferred Skills @ All Jobs Preferred Skills in All Jobs Word Frequency Word Frequency experience 4322 skills 3960 ability 3227 knowledge 1844 strong 1764 work 1541 management 1528 excellent 1509 communication 1494 written 1097 microsoft 1045 years 968 preferred 912 excel 880 project 830 working 829 new 750 verbal 699 data 693 organizational 669 5.5.2.3 Minium Qual Requirements @ Tech Jobs Minimum Qual Requirements in Technology Related Jobs Word Frequency Word Frequency experience 1827 computer 864 college 766 years 743 accredited 720 education 697 satisfactory 689 equivalent 628 described 575 fulltime 561 data 509 school 498 degree 472 four 457 programming 442 systems 429 least 386 months 383 high 356 related 352 5.5.2.4 Preferred Jobs @ Tech Jobs Preferred Skills in Technology Related Jobs Word Frequency Word Frequency experience 1030 skills 511 ability 460 knowledge 327 management 307 strong 295 years 270 security 227 excellent 206 development 202 working 189 communication 186 project 180 work 177 microsoft 172 systems 161 technical 156 data 154 sql 149 following 134 5.5.3 Obervations We can have plenty of observations from the four Word Clouds. For instance, we can see that for both Minimum Qual Requirements and Preferred Skills, experience is the most frequent word in all these four graphs, which makes sense, since previous working experience is indeed very important for applicants. Also, when comparing all jobs with technological jobs, we notice that for tech jobs prefer to hire employees with skills related to technology, since vocabularies like computer and programming appears a lot in these texts. Even some words about specific skills, such as sql, appear in our most frequent word list. Meanwhile, in all these four graphs, vocabularies like skills, knowledge, management, communication appear plenty of times. This makes sense since all employers want to hire people who have solid skills and are good at communication and cooperation. Finally, in general, we find that minimum requirements of all jobs and tech jobs graphs share almost the same set of frequent words, which we believe is due to the fact that minimum requirements are similar for all kinds of jobs. 5.6 More Studies on Tech Jobs From the bar plot we can see 79% of the technology job postings are full time jobs, and only 5% of them are part time. The remaining of them do not specify full time or part time. From this bar plot, we can see almost all (94%) technology jobs are annully paid, 5% of them are hourly paid, and only 1% are daily paid. To take a closer view, we look into the Job Count w.r.t Civil Service Title. From the plot above, we can see that for tech jobs, Computer System Manager is the most frequent. It occurs almost 50 times, whhich almost doubles the count of the second most title. The fewest civil service titles include Supervising Computer Service and Staff Analyst. Here we are looking at the relationship between job count and locations. From the bar plot above, we can see that most tech jobs in our data set are located at 255 greenwich street, 2 metro tech, and 355 adam street. After searching these locations on a map, we see that the first 10 locations in our plot are clustered. Therefore, we tend to believe that tech jobs are location sensitive. In other words, tech jobs are located within a certain area. More details can be found in an interactive map later in the book. This Cleveland dot plot shows the relationship between average annual salaries and civil service title. The salary ranges from less thatn 4000 dollars to almost 12000 dollars. We can also see that the annual salary of Aministrative Business Promot is a lot higher than any other jobs. "],
["interactive-component.html", "Chapter 6 Interactive Component", " Chapter 6 Interactive Component We created a shiny app to draw a interactive plot. The package that we used is called leaflet and it takes longitudes and latitudes of different data points to draw a map. Because a lot of data points have the exact longitudes and latitudes, if we plot the points directly, some points will be overlapping. In order to solve this issue, we applied a spider graph and all the points with the same address could be expanded if a user clicks on a circle. For this interactive plot, we added two selection bars and one slide bar. In that way, you can choose the category, agency and the range of the annual salary of the job postings. You can also click on the circles to see the exact job titles, annual salaries, etc. For the legend, it ranges from the lowest salary to the highest salary. In terms of the color of the legend, the lower a salary is, the more blue a point is; on the contrary, the higher a salary is, the redder a point is. For better viewing experience, you can also interact with our graph directly by clicking on this link https://jobpostingmap.shinyapps.io/5702-final-project/. "],
["conclusion.html", "Chapter 7 Conclusion 7.1 Lessons Learned 7.2 Summary of Studies the Dataset", " Chapter 7 Conclusion 7.1 Lessons Learned After completing this final project, we believe that we gained a better understanding of all the concepts that we have learned in this semester. Starting from researching online, we found this NYC Job Postings dataset and exported a .csv file for the use of our project. Then, leveraging the data processing skills we have learned, we cleaned up our data through properly dealing with NAs, removing duplicated elements that we did not need, and extracting more information such as locations by taking advantage of Google API. For the main part of project, we used many types of graphs we learnt in this semester, such as bar chart, box plot, histogram, Ridgeline plot, and Cleveland dot plot. We also took advantage of the knowledge learnt from the Community Contribution assignment and drew several Word Cloud graphs to showcase our skills in dealing with text data. Last but not least, for the interactive component, we utilized the Shiny app and created an interactive graph, from which users can check job salaries of the categories, agencies, or full/part time they select. Overall, we have learnt a lot through EDAV in this semester, and we really enjoy the process. 7.2 Summary of Studies the Dataset Here is a breif summary of some findings we got for the dataset. When compared full-time jobs with part-time jobs, overall, the distributions of two types of job, are not have a huge differences, but the amount of part-time jobs are greatly less than full-time jobs. Most of jobs with the salary lower than 150k/yr and the range of these jobs’ salaries are less than 50k. When the salary range over 50k, there is an obvious positive relationship between these two. Planning Maintenance and Producement Building Operations have a extremly high annual salary mean. Human Resource Finance and Health Policy are also obviously distinguished with others since their annual are over 100k. Most job categories with the salary between 60k to 80k dollars per year, the categories in this range are mostly related with technology, innovation, finance, and service. The most popular positions are all in this range, and the top 30 categories has the annual salary range between 100k to 200k dollars per year. For the salary group less than 60k, they also have low annual salary range, their sizes of needness online are also small. Detailed results can be found in the previous chapters. "]
]
